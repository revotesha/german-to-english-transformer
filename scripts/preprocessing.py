"""
Module for data preprocessing, tokenization, and utility functions.

Author: Revo Tesha (https://www.linkedin.com/in/revo-tesha/)
"""

import json

from datasets import Dataset
from datasets.formatting.formatting import LazyBatch

import torch
from torch.utils.data import DataLoader

from transformers import AutoTokenizer
from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast
from transformers.tokenization_utils_base import BatchEncoding

# <!> PLEASE NOTE that docstrings were generated by ChatGPT and may contain mistakes <!>


def generate_square_subsequent_mask(sz: int) -> torch.Tensor:
    """
    Generates a square subsequent mask to prevent attending to future tokens in a sequence.

    Args:
        sz (int): The size of the square mask, typically equal to the sequence length.

    Returns:
        Tensor: A tensor of shape (sz, sz) where the upper triangular part is filled with
            `-inf` to mask future positions, and the lower triangular (including diagonal)
            is filled with 0 to allow attention to current and previous positions.
    """

    # Note: I borrowed this code from the torch library
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = (
        mask.float()
        .masked_fill(mask == 0, float("-inf"))
        .masked_fill(mask == 1, float(0.0))
    )
    return mask


def read_corpus(filename: str) -> dict[str, list[str]]:
    """
    Reads a parallel corpus from a JSONL file and prepares sentences for translation tasks.

    Args:
        filename (str): Path to the JSONL file containing parallel sentences in two languages.
            Each line in the file is expected to be a JSON object with "de" (German) and "en"
            (English) keys for the respective sentences.

    Returns:
        dict[str, list[str]]: A dictionary with two keys:
            - "de": A list of German sentences with the [EOS] (end-of-sequence) token appended.
            - "en": A list of English sentences with the [BOS] (beginning-of-sequence) and [EOS]
            tokens added.
    """

    data = {"de": [], "en": []}
    with open(filename, "r", encoding="utf-8") as file:
        for line in file.readlines():
            json_line = json.loads(line)
            data["de"].append(json_line["de"] + " [EOS]")
            data["en"].append(
                "[BOS] " + json_line["en"] + " [EOS]"
            )  # shift target right
    return data


def tokenize(
    text: LazyBatch, tokenizer: BertTokenizerFast, language: str = "en"
) -> BatchEncoding:
    """
    Tokenizes a batch of sentences in a specific language using a given tokenizer.

    Args:
        text (LazyBatch): A batch of sentences to be tokenized. This is a lazy batch object
            from the Hugging Face datasets library, where the sentences are expected to be
            accessible by the given language key.
        tokenizer (BertTokenizerFast): The tokenizer used for tokenizing the sentences. It is
            expected to be a pre-trained BERT tokenizer.
        language (str, optional): The language key in the batch (e.g., "en" for English).
            Defaults to "en".

    Returns:
        BatchEncoding: A Hugging Face BatchEncoding object containing the tokenized output
            in PyTorch tensor format, with padding applied to match the max length and truncation
            as needed.
    """

    return tokenizer(
        text[language],
        add_special_tokens=False,
        max_length=100,  # attn masking and padding masking is easier when fixed
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    )


def get_tokenizers(
    de_model: str = "dbmdz/bert-base-german-cased",
    en_model: str = "bert-base-uncased",
) -> tuple[BertTokenizerFast, BertTokenizerFast, int, int]:
    """
    Loads pre-trained BERT tokenizers for German and English and adds special tokens.

    Args:
        de_model (str, optional): The model identifier for the German tokenizer.
            Defaults to "dbmdz/bert-base-german-cased".
        en_model (str, optional): The model identifier for the English tokenizer.
            Defaults to "bert-base-uncased".

    Returns:
        tuple[BertTokenizerFast, BertTokenizerFast, int, int]: A tuple containing:
            - The German tokenizer (BertTokenizerFast) with an added [EOS] token.
            - The English tokenizer (BertTokenizerFast) with added [BOS] and [EOS] tokens.
            - The vocabulary size of the German tokenizer (int) with the [EOS] token included.
            - The vocabulary size of the English tokenizer (int) with the [BOS] and [EOS] tokens included.
    """

    de_tokenizer = AutoTokenizer.from_pretrained(
        de_model, clean_up_tokenization_spaces=True
    )  # casing matters in German
    en_tokenizer = AutoTokenizer.from_pretrained(
        en_model, clean_up_tokenization_spaces=True
    )
    de_tokenizer.add_tokens(["[EOS]"], special_tokens=True)
    en_tokenizer.add_tokens(["[BOS]", "[EOS]"], special_tokens=True)

    de_vocab_size = de_tokenizer.vocab_size + 1  # count [EOS]
    en_vocab_size = en_tokenizer.vocab_size + 2  # count [BOS] & [EOS]

    return (de_tokenizer, en_tokenizer, de_vocab_size, en_vocab_size)


def get_data_loader(
    raw_data: Dataset,
    de_tokenizer: BertTokenizerFast,
    en_tokenizer: BertTokenizerFast,
    batch_size: int = 128,
) -> tuple[DataLoader, DataLoader]:
    """
    Prepares tokenized data loaders for both the source (German) and target (English) languages.

    Args:
        raw_data (Dataset): A HuggingFace `Dataset` containing raw parallel text data.
        de_tokenizer (BertTokenizerFast): Pretrained German tokenizer (BERT-based).
        en_tokenizer (BertTokenizerFast): Pretrained English tokenizer (BERT-based).
        batch_size (int, optional): Number of samples per batch. Defaults to 128.

    Returns:
        tuple[DataLoader, DataLoader]: A tuple containing:
            - DataLoader for the source (German) tokenized data.
            - DataLoader for the target (English) tokenized data.
    """

    src_data = raw_data.map(
        tokenize,
        batched=True,
        fn_kwargs={"tokenizer": de_tokenizer, "language": "de"},
    )
    trg_data = raw_data.map(
        tokenize,
        batched=True,
        fn_kwargs={"tokenizer": en_tokenizer, "language": "en"},
    )

    del raw_data

    src_data = src_data.remove_columns(["de", "en", "token_type_ids"])
    trg_data = trg_data.remove_columns(["de", "en", "token_type_ids"])

    src_data.set_format(type="torch", columns=["input_ids", "attention_mask"])
    trg_data.set_format(type="torch", columns=["input_ids", "attention_mask"])

    src_loader = DataLoader(src_data, batch_size=batch_size)
    trg_loader = DataLoader(trg_data, batch_size=batch_size)

    return src_loader, trg_loader
